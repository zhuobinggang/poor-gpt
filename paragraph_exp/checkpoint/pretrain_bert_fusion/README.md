## 实验结论

1. OK: 使用追加的[CLS]来训练attention模型比起用mean能够加速收束(不确定，因为也有learning rate的变量在，我调小了学习率在训练) -> 结论是彻底的错误，使用mean跟CLS效果理应是一样的，只是学习率影响很大
2. OK: 待实验(训练LSTM 6 Layer 1 Head, 如果性能比LSTM差，那就没什么好辩解的了，至少在少para设定的情况下是没办法超越LSTM的; 等等，先把dropout去掉): Trm vs LSTM，前者真的是个谎言吗？-> 不是谎言，但是由于可以学习的东西太微妙，且失去了顺序结构，所以需要更长的时间+更小的学习率来学习，而且需要更大的空间来储存信息。但是突破了临界点之后，理所当然的学到的东西比LSTM更多。 ->  基本上确定在9M的设定下TRM没办法击败LSTM, 可能重要的不是参数量，而是期待TRM在海量数据上学习到一些奇怪的东西，然后过来反哺回LSTM
3. OK: Attention Layer的Head比层数更重要？ 得先把LSTM的学习率也降低才能比较. (多头head并没有比单头性能更高，反而下降了一点点，可能是因为任务本身简单，可是将潜在空间划分区域不一定有效是我的想法。。) -> 单头，只要能训练就好
4. OK: BILSTM不知道为什么学习不到东西了，是因为我用了mean吗？还是因为Adapter?好像只是因为不需要那么小的训练率，训练的有些慢而已，稍微等一下吧，理论上肯定不能比0.55低才对。没事，就是训练慢了一点而已，没什么问题 -> 结论是BILSTM需要的learning rate比transformer更大
5. attention机制，能不能剪掉没必要的东西？ -> 看看论文


